<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>/posts/</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Tue, 19 May 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/posts/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>2Q(双链)缓存淘汰策略</title>
      <link>/posts/two-queue/</link>
      <pubDate>Tue, 19 May 2020 00:00:00 +0000</pubDate>
      
      <guid>/posts/two-queue/</guid>
      <description>LRU(最近使用)算法经常用于缓存应用中，最简单的实现是通过一个链表实现：
 插入数据时向头节点插入 更新数据时，移动节点到头节点 淘汰数据时删除尾节点  但是这个简单的实现并不能很好地应付很多场景，缓存的理想情况是预测未来数据的使用情况，尽可能的从缓存中读取数据，减少实际IO操作。
今天的记录是关于2个相似的LRU变种算法：LRU/2 和 2Q，以及它们的区别。
2Q(双链) 原始LRU算法描述的是数据使用的最近时间点，越靠近头节点的数据使用的时间点越近，但是没有描述数据使用的频率，像对于数据库的遍历操作，新数据会立即将缓存中的所有数据淘汰，但是遍历完后，缓存中的数据在之后使用的概率非常低，即缓存污染。
2Q淘汰算法是便是对以上情况的一种优化，淘汰策略是使用2个队列实现，1个FIFO队列记录只访问了一次的数据，1个普通LRU队列记录访问了2次以上的数据。
 当第1次访问时，将数据添加到FIFO队列，如果FIFO队列超过限制，淘汰FIFO里最旧的数据 当第2次访问时，将数据从FIFO队列移动到LRU队列的头节点，如果LRU队列超过限制，将LRU里最旧的数据移动到FIFO队列的头节点 当第3次以上访问时，按照LRU规则更新LRU队列  usestd::ptr::NonNull;usestd::fmt::Debug;pubstruct Node&amp;lt;T: Debug&amp;gt;{pubval: T,pubprev: Option&amp;lt;NonNull&amp;lt;Node&amp;lt;T&amp;gt;&amp;gt;&amp;gt;,pubnext: Option&amp;lt;NonNull&amp;lt;Node&amp;lt;T&amp;gt;&amp;gt;&amp;gt;,}pubstruct List&amp;lt;T: Debug&amp;gt;{pubhead: Option&amp;lt;NonNull&amp;lt;Node&amp;lt;T&amp;gt;&amp;gt;&amp;gt;,pubtail: Option&amp;lt;NonNull&amp;lt;Node&amp;lt;T&amp;gt;&amp;gt;&amp;gt;,publen: usize,}impl&amp;lt;T: Debug&amp;gt;List&amp;lt;T&amp;gt;{pubfn new()-&amp;gt; Self{Self{head: None,tail: None,len: 0,}}pubfn push_front(&amp;amp;mutself,val: T){letnode=Box::new(Node{val,prev: None,next: self.head,});letnode=NonNull::new(Box::into_raw(node));ifletSome(mutold_head)=self.head{unsafe{old_head.as_mut().prev=node;}}else{self.tail=node;}self.head=node;self.len+=1;}pubfn push_front_node(&amp;amp;mutself,node: NonNull&amp;lt;Node&amp;lt;T&amp;gt;&amp;gt;){letnode=Some(node);ifletSome(mutold_head)=self.head{unsafe{old_head.as_mut().prev=node;}}else{self.tail=node;}self.head=node;self.len+=1;}pubfn pop_back(&amp;amp;mutself)-&amp;gt; Option&amp;lt;NonNull&amp;lt;Node&amp;lt;T&amp;gt;&amp;gt;&amp;gt;{ifletSome(mutold_tail)=self.tail{unsafe{lettail=old_tail.as_mut().prev;ifletSome(muttail)=tail{tail.as_mut().next=None;}else{self.head=None;}self.tail=tail;self.len-=1;returnSome(old_tail);}}None}pubfn print(&amp;amp;self){letmutcur=self.head.as_ref();unsafe{whileletSome(c)=cur{letr=c.as_ref();println!(&amp;#34;{:?}&amp;#34;,r.val);cur=r.next.as_ref();}}}}struct TwoQueue{fifo: List&amp;lt;(String,i32)&amp;gt;,fifo_limit: usize,lru: List&amp;lt;(String,i32)&amp;gt;,lru_limit: usize,}implTwoQueue{fn new(fifo_limit: usize,lru_limit: usize)-&amp;gt; Self{Self{fifo: List::new(),fifo_limit,lru: List::new(),lru_limit,}}fn get(&amp;amp;mutself,key: &amp;amp;str)-&amp;gt; Option&amp;lt;i32&amp;gt;{letr=self.find_in_lru(key);ifr.is_some(){returnr;}self.find_in_fifo(key)}fn find_in_fifo(&amp;amp;mutself,key: &amp;amp;str)-&amp;gt; Option&amp;lt;i32&amp;gt;{letmutcur=self.fifo.head;whileletSome(c0)=cur{unsafe{letc=&amp;amp;mut*c0.as_ptr();ifc.val.0==key{// 取下 ifletSome(mutp)=c.prev{p.as_mut().next=c.next;}else{self.fifo.head=c.next;}ifletSome(mutn)=c.next{n.as_mut().prev=c.prev;}else{self.fifo.tail=c.prev;}self.fifo.len-=1;// 移动到lru队列 self.lru.push_front_node(c0);// 检查lru是否满 ifself.lru.len&amp;gt;self.lru_limit{ifletSome(node_from_lru)=self.lru.pop_back(){self.fifo.push_front_node(node_from_lru);}}// 返回 returnSome(c.val.1);}cur=c.next;}}None}fn find_in_lru(&amp;amp;mutself,key: &amp;amp;str)-&amp;gt; Option&amp;lt;i32&amp;gt;{letmutcur=self.</description>
    </item>
    
    <item>
      <title>Linux进程调度与定时器</title>
      <link>/posts/sched-and-timer/</link>
      <pubDate>Sun, 17 May 2020 20:14:50 +0800</pubDate>
      
      <guid>/posts/sched-and-timer/</guid>
      <description>由于Linux是属于抢占式(preemptoin)多任务(multitasking)分时操作系统，因此进程的调度同定时器必然存在联系，本篇日志是记录Linux进程调度与定时器的关系。
 Linux与Microsoft Windows等操作系统不同，并没有单独的线程机制，一组线程仅仅是共享了虚拟内存地址空间、打开的文件等资源的进程。
 Linux进程调度 操作系统的一个重要职责是将有限的资源通过特定的机制分配给多个用户使用，这里的资源包括CPU、内存、IO等，用户既可以指操作计算机的人，而人是给计算机下达任务的，因此更宽泛地指任务，也就是进程。
由于实际场景中，进程的数量是大于CPU处理器数量，多任务就是指同时并发地让进程交替使用CPU资源，让进程产生自己独占CPU的错觉，虚拟内存也是同理。
抢占(preemptoin)就是指不需要经过进程主动出让，内核调度器可以强制让进程让出CPU资源，然后去执行其他进程。
 与抢占式多任务相对应的，叫做协作式(cooperative)多任务，Go的goroutine便是一个应用范例。
 由于本篇日志的主题是进程调度与定时器的关系，所以抢占(preemptoin)便是这里的切入点。
由于进程正在执行代码，内核如果要去执行抢占操作，比如执行 schedule()，那必然需要去执行内核代码，而这里触发执行内核代码的其中之一，便是定时器中断。
不论是最早的Unix调度算法，2.5内核版本的O(1)调度算法，以及2.6之后出现的CFS完全公平调度算法，其中计算进程已经消耗的时间片(timeslice)都依赖于计算机的时间概念，而计算机的时间概念也是通过定时器实现的。
定时器 系统定时器是一种可编程硬件芯片，它能以固定的频率产生中断，这就是定时器中断。如果该中断信号没有被屏蔽，CPU便会去执行对应的中断处理程序，就可以去执行一些需要定时执行的代码，包括：
 更新系统运行的时间 更新实际时间 在SMP(对称多处理器)系统上，均衡各个处理器上的运行队列 检查当前进程是否用尽了时间片，如果用尽了则重新调度 运行已经超时的动态定时器 更新资源消耗、处理器时间的统计信息  系统在启动时，便会根据系统定时器的节拍率设置硬件。在x86体系结构中，系统定时器的默认节拍率是100HZ，也就是说每秒会触发100次定时器中断。该值可以自定义，越高产生中断的频率就越高，时钟中断的解析度也越高，像poll()和select()等系统调用的精度也越高，同理进程消耗的时间片计算和调度时机也更精确。
但是高节拍率也会带来副作用，意味着执行定时器中断处理程序的次数更多，这不但减少了执行其他任务的时间，同时还会打乱处理器的高速缓存（高速缓存依赖于空间和时间局部性）和增加耗电。</description>
    </item>
    
    <item>
      <title>DMA(直接内存访问)和零拷贝</title>
      <link>/posts/dma/</link>
      <pubDate>Sat, 16 May 2020 18:02:00 +0800</pubDate>
      
      <guid>/posts/dma/</guid>
      <description> 许多设备都可以临时控制总线。这些设备可以执行涉及主内存和其他设备的数据传送。由于设备执行这些操作的过程中无需借助于 CPU，因此该类型的数据传送称为直接内存访问 (direct memory access, DMA)。1
 大多数文件系统默认的IO操作都是缓存IO(Buffered I/O)，对于读(Read)操作，即IO设备先把数据发送到内核缓存区(Page Cache)，内核再将数据拷贝到应用程序地址空间的数据缓存区，而对于写(Write)操作，即反过来，从应用程序地址空间的数据缓存区拷贝到内核缓存区，内核再将数据发送到IO设备。
缓存IO的优势：
 可以利用内核缓存，如果数据已经在页缓存内，则不需要再读取IO设备，直接返回页缓存中的数据 对于写操作，应用程序只需要将数据拷贝到内核缓冲区即可返回，接下来应用程序缓冲区可以再次使用，而不需要等内核将数据写到IO设备(依赖于应用程序采用的写操作机制)。在一些场景下，比如应用程序每次只写1Byte，缓存IO可以将多次的写操作合并成一次IO写操作，有效地减少了IO操作次数，从而提供系统性能  缓存IO的劣势：
 数据从IO设备到应用程序地址空间需要经过内核缓冲区的中转，也就是拷贝操作，这些拷贝操作会消耗CPU，增加了系统负载。在某些场景下，比如网络文件服务，数据到达应用程序缓冲区后，又会原样的再次拷贝到内核缓冲区以发送到IO设备，这里的中转次数又增加了一倍 用于中转的缓冲区，不论是应用程序地址空间，还是内核空间，都会占用内存，加大了空间消耗  零拷贝 在一些场景下，我们需要减少数据拷贝的次数，以提高系统性能，零拷贝技术便是用来解决这个问题。在Linux操作系统中，有以下几种方式来实现零拷贝：
内存映射（mmap) mmap机制是先将数据从IO设备读取到内核缓冲区，然后通过应用程序地址空间和内核共享该内核缓冲区，这样就不需要拷贝了。
但是这里也会带来一次虚拟存储操作，而虚拟存储操作需要修改页表以及冲刷TLB来维持存储的一致性，这里的开销也不小，但是如果传输的数据较大，那还是值得的。
sendfile mmap会有虚拟转储开销，同时如果是发送网络数据的话，还需要把数据从内核缓冲区发送到socket缓冲区，最后发送到协议引擎中去，这里还是会有拷贝操作(内核拷贝)。sendfile机制可以再进一步的减少拷贝次数，同时避免虚拟转储操作。
sendfile利用DMA引擎将数据拷贝到内核缓冲区中，然后将带有文件位置和长度信息的缓冲区描述符添加到socket缓冲区中(这里不需要拷贝完整的数据)，DMA引擎将直接从内核缓冲区拷贝到协议引擎中去，这里避免了内核缓冲区到socket缓冲区的拷贝，同时没有映射内存。
DMA（直接内存访问） 前面提到了零拷贝技术中应用了DMA，可以跳过应用程序地址空间的中转，但是如果应用需要读取或修改数据呢？这时DMA也可以跳过内核缓冲区，实现数据从IO设备到用户地址空间的直接数据交换。
像数据库管理系统，希望自己管理页缓存，因为数据库知道自己存储的是什么数据，该如何换页等等。比如在提交事务时，需要redo log写入磁盘，才算事务提交完成，而buffer pool中的脏页并不需要立即写入磁盘，可以在换页时或者定时写入；在换页的时候，决定哪些页需要从buffer pool中换出，需要根据数据库自己的机制判断，依赖操作系统的换页机制将大大降低数据库系统的性能。
当然并不是所有地址空间都支持DMA，受限与硬件，在Linux中，只有ZONE_DMA区的内存可支持DMA操作，详情可参考Linux内存管理。
  第 9 章 直接内存访问 (Direct Memory Access, DMA) - docs.oracle.com &amp;#x21a9;&amp;#xfe0e;
   </description>
    </item>
    
  </channel>
</rss>